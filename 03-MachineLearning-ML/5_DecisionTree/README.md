
# ğŸŒ³ Decision Tree - Cheat Sheet

Quick reference guide for Data Science & ML interviews.

---

## 1ï¸âƒ£ What is Decision Tree?

> A supervised learning algorithm that makes predictions by learning decision rules from features, creating a tree-like flowchart structure.

**Used for:** Classification & Regression

---

## 2ï¸âƒ£ Key Terminology

| Term | Definition |
|------|------------|
| Root Node | First node (top), best feature split |
| Internal Node | Decision points with children |
| Leaf Node | Terminal node with final prediction |
| Depth | Longest path from root to leaf |
| Pruning | Cutting branches to reduce overfitting |

---

## 3ï¸âƒ£ Core Formulas

### Entropy
```
Entropy = -Î£ pi Ã— logâ‚‚(pi)

â€¢ 0 = Pure (all same class)
â€¢ 1 = Maximum impurity (50-50 split)
```

### Gini Impurity
```
Gini = 1 - Î£ piÂ²

â€¢ 0 = Pure
â€¢ 0.5 = Maximum impurity
```

### Information Gain
```
IG = Entropy(Parent) - Weighted Avg Entropy(Children)

â€¢ Higher IG = Better split
â€¢ Feature with highest IG â†’ Root Node
```

---

## 4ï¸âƒ£ How Tree Selects Root Node?

1. Calculate Information Gain for ALL features
2. Select feature with **HIGHEST** Information Gain
3. That feature becomes root node
4. Repeat process for child nodes

---

## 5ï¸âƒ£ Entropy vs Gini

| Aspect | Entropy | Gini |
|--------|---------|------|
| Formula | Uses log | Uses squares |
| Speed | Slower | Faster |
| Range | 0 to 1 | 0 to 0.5 |
| Sklearn Default | No | **Yes** |

---

## 6ï¸âƒ£ Overfitting

### Signs:
- Training Accuracy: 98%
- Test Accuracy: 60%
- **Large gap = Overfitting!**

### Solutions:

| Parameter | Change | Effect |
|-----------|--------|--------|
| max_depth | â†“ Decrease | Shorter tree |
| min_samples_split | â†‘ Increase | Fewer splits |
| min_samples_leaf | â†‘ Increase | Bigger leaves |
| max_leaf_nodes | â†“ Decrease | Fewer leaves |

---

## 7ï¸âƒ£ Pruning Types

| Type | When | How |
|------|------|-----|
| Pre-Pruning | Before building | Set max_depth, min_samples |
| Post-Pruning | After building | Remove unhelpful branches |

---

## 8ï¸âƒ£ Hyperparameters

```python
DecisionTreeClassifier(
    criterion='gini',      # 'gini' or 'entropy'
    max_depth=5,           # Limit depth
    min_samples_split=10,  # Min samples to split
    min_samples_leaf=5,    # Min samples in leaf
    max_leaf_nodes=20      # Max leaves
)
```

---

## 9ï¸âƒ£ Advantages & Disadvantages

| âœ… Pros | âŒ Cons |
|---------|---------|
| Easy to interpret | Overfitting prone |
| No feature scaling | Unstable |
| Handles all data types | Greedy (not optimal) |
| Shows feature importance | Biased with imbalanced data |
| Fast prediction | Single tree = less accurate |

---

## ğŸ”Ÿ Common Interview Questions

### Q1: How does Decision Tree select the best feature?
> By calculating Information Gain for all features and selecting the one with highest IG (reduces impurity most).

### Q2: Difference between Gini and Entropy?
> Both measure impurity. Gini uses squares (faster), Entropy uses log. Results are similar.

### Q3: How to prevent overfitting?
> Use pruning: limit max_depth, increase min_samples_split/leaf, limit max_leaf_nodes.

### Q4: Why is it called "greedy" algorithm?
> Makes best split at each step without considering future splits. May not find global optimum.

### Q5: When to use Decision Tree?
> When interpretability matters, data has non-linear relationships, mixed feature types.

### Q6: Decision Tree vs Random Forest?
> Random Forest = ensemble of many Decision Trees. More accurate but less interpretable.

---

## ğŸ“ Quick Code

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train
model = DecisionTreeClassifier(max_depth=5)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print(accuracy_score(y_test, y_pred))

# Feature Importance
print(model.feature_importances_)
```

---

## ğŸ¯ One-Liner Definitions

| Concept | One-Liner |
|---------|-----------|
| Decision Tree | Tree-based model that splits data using feature rules |
| Entropy | Measure of randomness/impurity in data |
| Gini | Alternative impurity measure using squared probabilities |
| Information Gain | Reduction in impurity after split |
| Pruning | Cutting tree branches to prevent overfitting |
| Overfitting | Model memorizes training data, fails on new data |

---

## âš¡ Last-Minute Revision

```
REMEMBER:
â”œâ”€â”€ Impurity: Entropy & Gini
â”œâ”€â”€ Split by: Highest Information Gain
â”œâ”€â”€ Overfitting: Big gap Train vs Test
â”œâ”€â”€ Fix Overfitting: â†“max_depth, â†‘min_samples
â”œâ”€â”€ Sklearn default: criterion='gini'
â””â”€â”€ Tree is GREEDY (local optimal, not global)
```

---

Good luck with your interview! ğŸš€
```

---

# âœ… README Complete!

| Section | Covered |
|---------|---------|
| Definition | âœ… |
| Terminology | âœ… |
| Formulas | âœ… |
| How it works | âœ… |
| Overfitting | âœ… |
| Hyperparameters | âœ… |
| Pros/Cons | âœ… |
| Interview Q&A | âœ… |
| Quick Code | âœ… |
| One-liners | âœ… |

---

# ğŸ¯ Ready for Naive Bayes?

Now let's continue! Answer the 3 tasks:

**Task 1:** Match: 1-?, 2-?, 3-?, 4-?
```
1. Prior      A. P(B|A)
2. Likelihood B. P(A|B)  
3. Posterior  C. P(A)
4. Evidence   D. P(B)
```

**Task 2:** Calculate P(PASS | Studied)

**Task 3:** If P(SPAM|FREE) = 80.4%, what is P(NOT SPAM|FREE)?