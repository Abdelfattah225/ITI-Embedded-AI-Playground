# ğŸ§  Deep Dive: Keras/TensorFlow Learning Session



## ğŸ“¦ 1. IMPORTS EXPLAINED

```python
from tensorflow.keras.models import Sequential      # Model architecture type
from tensorflow.keras.layers import Dense, Dropout  # Layer types
from tensorflow.keras.regularizers import l2        # Regularization technique
from tensorflow.keras.optimizers import Adam        # Optimization algorithm
from sklearn.datasets import make_classification    # Generate fake data
from sklearn.model_selection import train_test_split # Split data
```

---

## ğŸ—ï¸ 2. SEQUENTIAL MODEL

```python
model = Sequential()
```

### What is Sequential?
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEQUENTIAL MODEL                      â”‚
â”‚                                                          â”‚
â”‚   Data flows in ONE direction: Input â†’ Output           â”‚
â”‚                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚ Layer 1 â”‚ â†’  â”‚ Layer 2 â”‚ â†’  â”‚ Layer 3 â”‚ â†’ Output   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚   Like a stack of pancakes - one on top of another!     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When to use Sequential:**
- Simple feed-forward networks
- Each layer has exactly ONE input and ONE output

**When NOT to use Sequential:**
- Multiple inputs/outputs
- Layer sharing
- Non-linear topology

---

## ğŸ”² 3. DENSE LAYER (Fully Connected)

```python
Dense(64, activation='relu', input_shape=(100,), kernel_regularizer=l2(0.01))
```

### Visual Representation:
```
INPUT LAYER (100 features)          DENSE LAYER (64 neurons)
        
    â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â—‹
    â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â—‹
    â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â—‹
    â—‹ â”€â”€â”€â”€â”€â”€ ALL CONNECTED TO ALL â”€â”€â”€â”€ â—‹
    â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â—‹
   ...                                ...
    â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â—‹
   
 (100 inputs)                      (64 outputs)
 
 Total connections = 100 Ã— 64 = 6,400 weights!
 Plus 64 biases = 6,464 parameters
```

### Parameters Breakdown:

| Parameter | Your Value | Meaning |
|-----------|------------|---------|
| `units` | 64 | Number of neurons in this layer |
| `activation` | 'relu' | Activation function applied |
| `input_shape` | (100,) | Shape of input data (only for first layer) |
| `kernel_regularizer` | l2(0.01) | Regularization to prevent overfitting |

---

## âš¡ 4. ACTIVATION FUNCTIONS

### ReLU (Rectified Linear Unit)
```python
activation='relu'
```

```
         â”‚ output
       4 â”‚        â•±
       3 â”‚       â•±
       2 â”‚      â•±
       1 â”‚     â•±
    â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€ input
      -2 â”‚   0   2   4
         â”‚
         
Formula: f(x) = max(0, x)

â€¢ If x > 0: output = x
â€¢ If x â‰¤ 0: output = 0
```

**Why ReLU?**
- âœ… Fast computation
- âœ… Reduces vanishing gradient problem
- âœ… Introduces non-linearity
- âš ï¸ Can cause "dying ReLU" (neurons always output 0)

### Softmax (For Output Layer)
```python
activation='softmax'
```

```
Raw scores (logits):     After Softmax:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Class 0: 2 â”‚         â”‚  Class 0: 0.09 â”‚  (9%)
â”‚  Class 1: 5 â”‚    â†’    â”‚  Class 1: 0.87 â”‚  (87%)  â† Predicted!
â”‚  Class 2: 1 â”‚         â”‚  Class 2: 0.04 â”‚  (4%)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        Sum = 1.00 (100%)

Formula: softmax(xáµ¢) = e^xáµ¢ / Î£(e^xâ±¼)
```

**Why Softmax for classification?**
- Converts raw scores to probabilities
- All outputs sum to 1
- Highest probability = predicted class

---

## ğŸ›¡ï¸ 5. REGULARIZATION TECHNIQUES

### L2 Regularization (Ridge)
```python
kernel_regularizer=l2(0.01)
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     L2 REGULARIZATION                       â”‚
â”‚                                                             â”‚
â”‚   Original Loss = Prediction Error                          â”‚
â”‚                                                             â”‚
â”‚   New Loss = Prediction Error + Î» Ã— Î£(weightsÂ²)            â”‚
â”‚                                    â†‘                        â”‚
â”‚                              Penalty term                   â”‚
â”‚                                                             â”‚
â”‚   Î» = 0.01 (your value) - controls penalty strength        â”‚
â”‚                                                             â”‚
â”‚   Effect: Pushes weights toward SMALLER values             â”‚
â”‚           Prevents any single weight from dominating        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dropout
```python
Dropout(0.5)
```

```
TRAINING MODE (Dropout Active):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                          â”‚
â”‚   â—‹ â”€â”€â”€â”€ â—‹        â—‹ â”€â”€â”€â”€ â—‹              â”‚
â”‚   â—‹ â”€â”€â”€â”€ â•³ (OFF)  â—‹ â”€â”€â”€â”€ â—‹              â”‚
â”‚   â—‹ â”€â”€â”€â”€ â—‹        â—‹ â”€â”€â”€â”€ â•³ (OFF)        â”‚
â”‚   â—‹ â”€â”€â”€â”€ â•³ (OFF)  â—‹ â”€â”€â”€â”€ â—‹              â”‚
â”‚   â—‹ â”€â”€â”€â”€ â—‹        â—‹ â”€â”€â”€â”€ â—‹              â”‚
â”‚                                          â”‚
â”‚   50% of neurons randomly "turned off"   â”‚
â”‚   each training step                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INFERENCE MODE (Dropout Inactive):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   All neurons active, weights scaled     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Dropout works:**
- Forces network to learn redundant representations
- Prevents co-adaptation of neurons
- Acts like training multiple networks

---

## ğŸ¯ 6. ADAM OPTIMIZER

```python
optimizer=Adam(learning_rate=0.001)
```

### Full Parameters:
```python
Adam(
    learning_rate=0.001,  # Step size
    beta_1=0.9,           # Momentum decay rate
    beta_2=0.999,         # RMSprop decay rate
    amsgrad=False         # Variant for better convergence
)
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADAM = Adaptive Moment Estimation        â”‚
â”‚                                                             â”‚
â”‚   Combines two techniques:                                  â”‚
â”‚                                                             â”‚
â”‚   1. MOMENTUM (Î²â‚ = 0.9)                                   â”‚
â”‚      â”œâ”€â”€ Remembers past gradients                          â”‚
â”‚      â””â”€â”€ Helps escape local minima                         â”‚
â”‚                                                             â”‚
â”‚   2. RMSprop (Î²â‚‚ = 0.999)                                  â”‚
â”‚      â”œâ”€â”€ Adapts learning rate per parameter                â”‚
â”‚      â””â”€â”€ Larger updates for infrequent features            â”‚
â”‚                                                             â”‚
â”‚   Learning Rate = 0.001 (default, usually good start)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimizer Comparison:
```
              Learning Rate Adaptation    Momentum
SGD           âœ—                           âœ—
SGD+Momentum  âœ—                           âœ“
RMSprop       âœ“                           âœ—
Adam          âœ“                           âœ“  â† Best of both!
```

---

## ğŸ“‰ 7. LOSS FUNCTION

```python
loss='categorical_crossentropy'
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CATEGORICAL CROSS-ENTROPY                      â”‚
â”‚                                                             â”‚
â”‚   True Label:      [0, 1, 0]  (Class 1)                    â”‚
â”‚   Prediction:      [0.1, 0.8, 0.1]                         â”‚
â”‚                                                             â”‚
â”‚   Loss = -Î£(yáµ¢ Ã— log(Å·áµ¢))                                  â”‚
â”‚        = -(0Ã—log(0.1) + 1Ã—log(0.8) + 0Ã—log(0.1))          â”‚
â”‚        = -log(0.8)                                          â”‚
â”‚        = 0.223                                              â”‚
â”‚                                                             â”‚
â”‚   Perfect prediction â†’ Loss = 0                             â”‚
â”‚   Wrong prediction   â†’ Loss = âˆ                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Which Loss to Use?

| Problem Type | Output Activation | Loss Function |
|--------------|-------------------|---------------|
| Binary (2 classes) | sigmoid | `binary_crossentropy` |
| Multi-class (labels one-hot) | softmax | `categorical_crossentropy` |
| Multi-class (labels integers) | softmax | `sparse_categorical_crossentropy` |
| Regression | linear | `mse` or `mae` |

---

## ğŸ”§ 8. MODEL COMPILATION

```python
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPILATION STEP                         â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚  OPTIMIZER  â”‚ â†’ HOW to update weights                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚    LOSS     â”‚ â†’ WHAT to minimize                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚   METRICS   â”‚ â†’ WHAT to monitor (doesn't affect        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   training, just for reporting)          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š 9. YOUR TWO ARCHITECTURES COMPARED

### Architecture 1:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input (100) â†’ Dense(64, relu, L2) â†’ Dense(32, relu, L2)   â”‚
â”‚             â†’ Dense(10, softmax)                            â”‚
â”‚                                                             â”‚
â”‚  Total params: ~10,000                                      â”‚
â”‚  Regularization: L2 only                                    â”‚
â”‚  Use case: Simpler tasks, less data                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture 2:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input (784) â†’ Dense(128, relu, L2) â†’ Dropout(0.5)         â”‚
â”‚             â†’ Dense(64, relu, L2)  â†’ Dropout(0.5)          â”‚
â”‚             â†’ Dense(10, softmax)                            â”‚
â”‚                                                             â”‚
â”‚  Total params: ~109,000                                     â”‚
â”‚  Regularization: L2 + Dropout (double protection!)          â”‚
â”‚  Use case: MNIST-like data (28Ã—28 images = 784 pixels)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ 10. DATA PREPARATION

```python
# Generate synthetic classification data
x, y = make_classification(
    n_samples=1000,      # Total examples
    n_features=784,      # Input dimensions
    n_classes=10,        # Output classes
    n_informative=50,    # Features that actually matter
    random_state=42      # Reproducibility
)

# Split into train/test
x_train, x_test, y_train, y_test = train_test_split(
    x, y,
    test_size=0.2,       # 20% for testing
    random_state=42
)
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA SPLIT                              â”‚
â”‚                                                             â”‚
â”‚   Original Data (1000 samples)                              â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•          â”‚
â”‚                                                             â”‚
â”‚   â”œâ”€â”€ Training Set (800 samples, 80%)                      â”‚
â”‚   â”‚   â””â”€â”€ Model learns from this                           â”‚
â”‚   â”‚                                                         â”‚
â”‚   â””â”€â”€ Test Set (200 samples, 20%)                          â”‚
â”‚       â””â”€â”€ Evaluate final performance                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ Issue in Your Code!
```python
# Your output labels (y) are integers: [0, 1, 2, ..., 9]
# But you're using categorical_crossentropy which expects one-hot encoding!

# Fix Option 1: Convert to one-hot
from tensorflow.keras.utils import to_categorical
y_train_encoded = to_categorical(y_train, num_classes=10)
y_test_encoded = to_categorical(y_test, num_classes=10)

# Fix Option 2: Use sparse loss (simpler!)
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',  # Works with integer labels
    metrics=['accuracy']
)
```

---

## ğŸ“ COMPLETE WORKFLOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DEEP LEARNING WORKFLOW                     â”‚
â”‚                                                              â”‚
â”‚   1. PREPARE DATA                                            â”‚
â”‚      â””â”€â”€ Load â†’ Clean â†’ Split â†’ Normalize                   â”‚
â”‚                                                              â”‚
â”‚   2. BUILD MODEL                                             â”‚
â”‚      â””â”€â”€ Define architecture (Sequential + Layers)          â”‚
â”‚                                                              â”‚
â”‚   3. COMPILE MODEL                                           â”‚
â”‚      â””â”€â”€ Optimizer + Loss + Metrics                         â”‚
â”‚                                                              â”‚
â”‚   4. TRAIN MODEL (you haven't done this yet!)               â”‚
â”‚      â””â”€â”€ model.fit(x_train, y_train, epochs, batch_size)    â”‚
â”‚                                                              â”‚
â”‚   5. EVALUATE MODEL                                          â”‚
â”‚      â””â”€â”€ model.evaluate(x_test, y_test)                     â”‚
â”‚                                                              â”‚
â”‚   6. PREDICT                                                 â”‚
â”‚      â””â”€â”€ model.predict(new_data)                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ¯ YOUR TASK: Build a Complete DL Model

## Task: Binary Sentiment Classifier

**Scenario:** Build a neural network to classify movie reviews as positive (1) or negative (0).

### Requirements:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TASK SPECIFICATIONS                  â”‚
â”‚                                                              â”‚
â”‚   INPUT: 500 features (simulating word embeddings)          â”‚
â”‚   OUTPUT: Binary classification (positive/negative)         â”‚
â”‚                                                              â”‚
â”‚   ARCHITECTURE REQUIREMENTS:                                 â”‚
â”‚   âœ“ At least 3 hidden layers                                â”‚
â”‚   âœ“ Use BOTH Dropout AND L2 regularization                  â”‚
â”‚   âœ“ Decreasing neuron pattern (e.g., 256â†’128â†’64)           â”‚
â”‚   âœ“ Appropriate activation functions                        â”‚
â”‚   âœ“ Proper output layer for binary classification           â”‚
â”‚                                                              â”‚
â”‚   MUST INCLUDE:                                              â”‚
â”‚   âœ“ Generate synthetic data using make_classification       â”‚
â”‚   âœ“ Split into train/test (80/20)                          â”‚
â”‚   âœ“ Compile with appropriate loss                           â”‚
â”‚   âœ“ Train for 50 epochs with batch_size=32                 â”‚
â”‚   âœ“ Print final test accuracy                               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Starter Template:

```python
# YOUR IMPORTS HERE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# STEP 1: Generate Data
# Create binary classification data with 500 features
# n_samples=2000, n_features=500, n_classes=2
# YOUR CODE HERE


# STEP 2: Split Data
# 80% train, 20% test
# YOUR CODE HERE


# STEP 3: Build Model
model = Sequential()
# Add your layers here
# Layer 1: 256 neurons, relu, L2(0.001), input_shape=(500,)
# YOUR CODE HERE

# Layer 2: 128 neurons + Dropout
# YOUR CODE HERE

# Layer 3: 64 neurons
# YOUR CODE HERE

# Output Layer: ??? neurons, ??? activation
# YOUR CODE HERE


# STEP 4: Compile
# Choose the RIGHT loss for binary classification!
# YOUR CODE HERE


# STEP 5: Train
# epochs=50, batch_size=32, validation_split=0.2
# YOUR CODE HERE


# STEP 6: Evaluate
# Print test accuracy
# YOUR CODE HERE
```

### Checklist Before Submission:

- [ ] Used `sigmoid` activation for binary output
- [ ] Used `binary_crossentropy` loss
- [ ] Applied L2 regularization to hidden layers
- [ ] Added Dropout between layers
- [ ] Model compiles without errors
- [ ] Model trains and shows decreasing loss
- [ ] Test accuracy is above 70%

---

## ğŸ† BONUS CHALLENGES (Optional)

1. **Add Early Stopping** to prevent overfitting:
```python
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=5)
model.fit(..., callbacks=[early_stop])
```

2. **Add Learning Rate Scheduler**
3. **Plot training vs validation loss**
4. **Try different Dropout rates (0.2, 0.3, 0.5) and compare**

---

Good luck! ğŸ’ª Once you complete this task, you'll have solid fundamentals to build real-world deep learning models. Let me know if you need any hints!